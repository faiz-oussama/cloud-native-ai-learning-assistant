"""
RAG Chat Service using Azure OpenAI and Azure AI Search

This module implements a Retrieval Augmented Generation (RAG) service that connects
Azure OpenAI with Azure AI Search. RAG enhances LLM responses by grounding them in
your enterprise data stored in Azure AI Search.
"""
import logging
from typing import List
from azure.core.credentials import AzureKeyCredential
from openai import AsyncAzureOpenAI
from app.models.chat_models import ChatMessage, QueryRequest, QueryResponse
from app.config import settings

logger = logging.getLogger(__name__)


class RagChatService:
    """
    Service that provides Retrieval Augmented Generation (RAG) capabilities
    by connecting Azure OpenAI with Azure AI Search for grounded responses.
    
    This service:
    1. Handles authentication to Azure services using API keys
    2. Implements the "On Your Data" pattern using Azure AI Search as a data source
    3. Processes user queries and returns AI-generated responses grounded in your data
    """
    
    def __init__(self):
        """Initialize the RAG chat service using settings from app config"""
        # Store settings for easy access
        self.openai_endpoint = settings.azure_openai_endpoint
        self.gpt_deployment = settings.azure_openai_gpt_deployment
        self.embedding_deployment = settings.azure_openai_embedding_deployment
        self.search_url = settings.azure_search_service_url
        self.search_index_name = settings.azure_search_index_name
        self.system_prompt = settings.system_prompt
        
        # Get API keys from environment variables
        import os
        self.foundry_key = os.getenv("FOUNDRY_KEY")
        self.search_key = os.getenv("SEARCH_SERVICE_KEY")
        
        # Validate that we have the required keys
        if not self.foundry_key:
            raise ValueError("FOUNDRY_KEY environment variable is required")
        if not self.search_key:
            raise ValueError("SEARCH_SERVICE_KEY environment variable is required")
        
        # Create Azure OpenAI client with API key authentication
        self.openai_client = AsyncAzureOpenAI(
            azure_endpoint=self.openai_endpoint,
            api_key=self.foundry_key,
            api_version=settings.azure_openai_api_version
        )
        
        logger.info("RagChatService initialized with environment variables")
    
    async def get_chat_completion(self, history: List[ChatMessage]):
        """
        Process a chat completion request with RAG capabilities by integrating with Azure AI Search
        
        This method:
        1. Formats the conversation history for Azure OpenAI
        2. Configures Azure AI Search as a data source using the "On Your Data" pattern
        3. Sends the request to Azure OpenAI with data_sources parameter
        4. Returns the response with citations to source documents
        
        Args:
            history: List of chat messages from the conversation history
            
        Returns:
            Raw response from the OpenAI API with citations from Azure AI Search
        """
        try:
            # Limit chat history to the 20 most recent messages to prevent token limit issues
            recent_history = history[-20:] if len(history) > 20 else history
            
            # Convert to Azure OpenAI compatible message format
            messages = []
            
            # Add system message
            messages.append({
                "role": "system", 
                "content": self.system_prompt
            })
            
            # Add conversation history
            for msg in recent_history:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # Configure Azure AI Search data source according to the "On Your Data" pattern
            # This connects Azure OpenAI directly to your search index without needing to
            # manually implement vector search, chunking, or semantic rankers
            data_source = {
                "type": "azure_search",
                "parameters": {
                    "endpoint": self.search_url,
                    "index_name": self.search_index_name,
                    "authentication": {
                        "type": "api_key",
                        "key": self.search_key
                    },
                    # Combines vector and traditional search
                    "query_type": "vector_semantic_hybrid",
                    # The naming pattern for semantic configuration is generated by Azure AI Search 
                    # during integrated vectorization and cannot be customized
                    "semantic_configuration": f"{self.search_index_name}-semantic-configuration",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": self.embedding_deployment
                    }
                }
            }
            
            # Call Azure OpenAI for completion with the data_sources parameter directly
            # The data_sources parameter enables the "On Your Data" pattern, where
            # Azure OpenAI automatically retrieves relevant documents from your search index
            response = await self.openai_client.chat.completions.create(
                model=self.gpt_deployment,
                messages=messages,
                extra_body={
                    "data_sources": [data_source]
                },
                stream=False
            )
            
            # Return the raw response
            return response
            
        except Exception as e:
            logger.error(f"Error in get_chat_completion: {str(e)}")
            # Propagate all errors to the controller layer
            raise
    
    async def query_documents(self, request: QueryRequest):
        """
        Query documents using RAG pattern by integrating with Azure AI Search
        
        This method:
        1. Creates a chat message from the query
        2. Uses the get_chat_completion method to process the query with RAG
        3. Returns the response with citations to source documents
        
        Args:
            request: Query request containing the user's question and context
            
        Returns:
            QueryResponse with the answer from the RAG service
        """
        try:
            # Create a chat message from the query
            chat_message = ChatMessage(
                role="user",
                content=request.query
            )
            
            # Process with RAG
            response = await self.get_chat_completion([chat_message])
            
            # Extract the answer from the response
            answer = "I couldn't generate an answer from the document."
            if response.choices and len(response.choices) > 0:
                choice = response.choices[0]
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    answer = choice.message.content
                elif isinstance(choice, dict) and 'message' in choice and 'content' in choice['message']:
                    answer = choice['message']['content']
            
            # Return the response in the expected format
            return QueryResponse(
                answer=answer,
                query=request.query,
                model_used=self.gpt_deployment
            )
            
        except Exception as e:
            logger.error(f"Error in query_documents: {str(e)}")
            # Return a default response on error
            return QueryResponse(
                answer="I apologize, but I'm currently unable to answer questions about the document. The AI service may be temporarily unavailable. Please try again later.",
                query=request.query,
                model_used=self.gpt_deployment
            )


# Create singleton instance
rag_chat_service = RagChatService()